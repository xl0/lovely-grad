# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_repr_str.ipynb.

# %% auto 0
__all__ = ['lovely']

# %% ../nbs/00_repr_str.ipynb 4
import warnings
from typing import Union, Optional as O

import numpy as np
# import jax, jax.numpy as jnp

from lovely_numpy import np_to_str_common, pretty_str, sparse_join, ansi_color, in_debugger, bytes_to_human
from lovely_numpy import config as lnp_config

from .utils.config import get_config, config, set_config
from .utils.misc import is_cpu

import tinygrad.helpers, tinygrad.tensor
from tinygrad.tensor import Tensor, DType, dtypes

# %% ../nbs/00_repr_str.ipynb 6
dtnames =   {   "half": "f16",
                "float": "f32",
                "char": "i8",
                "uchar": "u8",
                "int":   "i32",
                "int64": "i64",
            }


def short_dtype(x: DType) -> str:
    return dtnames.get(x.dtype.name, str(x.dtype)) if x.dtype != Tensor.default_type else ""

# %% ../nbs/00_repr_str.ipynb 8
def plain_repr(x: Tensor):
    "Pick the right function to get a plain repr"
    # assert isinstance(x, np.ndarray), f"expected np.ndarray but got {type(x)}" # Could be a sub-class.
    return x._plain_repr() if hasattr(x, "_plain_repr") else repr(x)


# %% ../nbs/00_repr_str.ipynb 9
def is_nasty(x: Tensor):
    """Return true of any `x` values are inf or nan"""
    if x.shape == (): return False # min/max don't like zero-lenght arrays
    
    x_min = x.min().numpy().squeeze()
    x_max = x.max().numpy().squeeze()

    return np.isnan(x_min) or np.isinf(x_min) or np.isinf(x_max)

# %% ../nbs/00_repr_str.ipynb 12
def tensor_to_str_common(x: Tensor,  # Input
                        color=True,  # ANSI color highlighting
                        ddof=0):     # For "std" unbiasing

    if x.numel() == 0: return ansi_color("empty", "grey", color)
    if ((x == 0).min() == 1).numpy(): return ansi_color("all_zeros", "grey", color)

    if x.ndim > 0:
        x_min = x.min().numpy().squeeze()
        x_max = x.max().numpy().squeeze()
        minmax = f"x∈[{pretty_str(x_min)}, {pretty_str(x_max)}]" if x.numel() > 2 else None

        # XXX Add bias correction?
        x_mean = x.mean().numpy().squeeze()
        x_std = x.std().numpy().squeeze()
        meanstd = f"μ={pretty_str(x_mean)} σ={pretty_str(x_std)}" if x.numel() >= 2 else None

        return sparse_join([minmax, meanstd])

# %% ../nbs/00_repr_str.ipynb 13
def to_str(x: Tensor,  # Input
            verbose:        bool    =False,
            auto_realize:   O[bool] =None,
            depth:          int     =0,
            lvl:            int     =0,
            color:          O[bool] =None
        ) -> str:

    # if plain:
    #     return plain_repr(x)

    conf = get_config()
    if color is None: color=conf.color
    if auto_realize is None: auto_realize=conf.auto_realize

    if in_debugger(): color = False


    tname = type(x).__name__.split(".")[-1]             # Tensor
    shape = str(list(x.shape)) if (x.ndim) else None   # [1,2,3]
    type_str = sparse_join([tname, shape], sep="")      # Tensor[1,2,3]

    dtype = short_dtype(x)                              # f16
    dev = x.device                                      # CPU

    grad = "grad" if x.requires_grad else None          # grad
    if x.grad is not None: grad = grad + ansi_color("+", "green", color)


    numel = None
    if x.shape and max(x.shape) != x.numel():
        numel = f"n={x.numel()}"
        # if get_config().show_mem_above <= x.nbytes:
        #     numel = sparse_join([numel, f"({bytes_to_human(x.nbytes)})"])
    # elif get_config().show_mem_above <= x.nbytes:
        # numel = bytes_to_human(x.nbytes)

    res  = ""
    if verbose: # Put this on top before the tensor is possibly realized.
        res += plain_repr(x) + "\n"

    just_realized = None
    if auto_realize and not x.lazydata.realized:
        just_realized = ansi_color("Realized "+ str(x.lazydata.op.op).split(".")[-1], "grey", color)
        x.realize()

    if x.lazydata.realized:
        # `lovely-numpy` is used to calculate stats when doing so on GPU would require
        # memory allocation (no-float tensors, tensors with bad numbers),
        #
        # Temporarily set the numpy config to match our config for consistency.
        with lnp_config(precision=conf.precision,
                        threshold_min=conf.threshold_min,
                        threshold_max=conf.threshold_max,
                        sci_mode=conf.sci_mode):

            if is_nasty(x) or not x.is_floating_point():
                common = np_to_str_common(x.numpy(), color=color)
            else:
                common = tensor_to_str_common(x, color=color)

            vals = pretty_str(x.numpy()) if 0 < x.numel() <= 10 else None
            res += sparse_join([type_str, dtype, numel, common, grad, dev, just_realized, vals])
    else:
        op = "Lazy " + str(x.lazydata.op.op).split(".")[-1]
        res += sparse_join([type_str, dtype, numel, grad, dev, op])
    # else:
    #     res = plain_repr(x)


    if depth and x.ndim > 1:
        with config(show_mem_above=np.inf):
            deep_width = min((x.shape[0]), conf.deeper_width) # Print at most this many lines
            deep_lines = [ " "*conf.indent*(lvl+1) + to_str(x[i,:].realize(), depth=depth-1, lvl=lvl+1)
                                for i in range(deep_width)] 

            # If we were limited by width, print ...
            if deep_width < x.shape[0]: deep_lines.append(" "*conf.indent*(lvl+1) + "...")

            res += "\n" + "\n".join(deep_lines)

    return res

# %% ../nbs/00_repr_str.ipynb 14
def history_warning():
    "Issue a warning (once) ifw e are running in IPYthon with output cache enabled"

    if "get_ipython" in globals() and get_ipython().cache_size > 0:
        warnings.warn("IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html")

# %% ../nbs/00_repr_str.ipynb 17
class StrProxy():
    def __init__(self, x: Tensor, plain=False, verbose=False, depth=0, lvl=0, color=None):
        self.x = x
        self.plain = plain
        self.verbose = verbose
        self.depth=depth
        self.lvl=lvl
        self.color=color
        history_warning()
    
    def __repr__(self):
        if self.plain: return plain_repr(self.x)
        return to_str(self.x, verbose=self.verbose,
                      depth=self.depth, lvl=self.lvl, color=self.color)

    # This is used for .deeper attribute and .deeper(depth=...).
    # The second onthe results in a __call__.
    def __call__(self, depth=1):
        return StrProxy(self.x, depth=depth)

# %% ../nbs/00_repr_str.ipynb 18
def lovely(x: Tensor, # Tensor of interest
            verbose=False,  # Whether to show the full tensor
            depth=0,        # Show stats in depth
            color=None):    # Force color (True/False) or auto.
    return StrProxy(x, verbose=verbose, depth=depth, color=color)
